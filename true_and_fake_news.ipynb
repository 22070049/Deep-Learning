{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 4679796,
          "sourceType": "datasetVersion",
          "datasetId": 2712039
        },
        {
          "sourceId": 323555,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 272577,
          "modelId": 293554
        },
        {
          "sourceId": 323752,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 272737,
          "modelId": 293711
        }
      ],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/22070049/Deep-Learning/blob/main/true_and_fake_news.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:29:50.379887Z",
          "iopub.execute_input": "2025-04-06T15:29:50.380198Z",
          "iopub.status.idle": "2025-04-06T15:29:50.709487Z",
          "shell.execute_reply.started": "2025-04-06T15:29:50.380177Z",
          "shell.execute_reply": "2025-04-06T15:29:50.708582Z"
        },
        "id": "jN0TRArHtgVf"
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip the file\n",
        "zip_path = '/content/archive (5).zip'\n",
        "extract_path = '/tmp/fake-news-detection-datasets'\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# Load datasets\n",
        "true_df = pd.read_csv(os.path.join(extract_path, 'News _dataset', 'True.csv'))\n",
        "fake_df = pd.read_csv(os.path.join(extract_path, 'News _dataset', 'Fake.csv'))\n",
        "\n",
        "\n",
        "# Add labels\n",
        "true_df['label'] = 1  # Real news\n",
        "fake_df['label'] = 0  # Fake news\n",
        "\n",
        "# Combine datasets\n",
        "df = pd.concat([true_df, fake_df], ignore_index=True)\n",
        "\n",
        "# Shuffle\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:02.160298Z",
          "iopub.execute_input": "2025-04-06T15:30:02.160722Z",
          "iopub.status.idle": "2025-04-06T15:30:04.892555Z",
          "shell.execute_reply.started": "2025-04-06T15:30:02.160699Z",
          "shell.execute_reply": "2025-04-06T15:30:04.891663Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "nIms1FgTtgVi",
        "outputId": "eba082b6-5d64-41bf-8570-ad7a6c4b923a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/archive (5).zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-771895730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mzip_ref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextract_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/zipfile/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/archive (5).zip'"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:09.02462Z",
          "iopub.execute_input": "2025-04-06T15:30:09.024943Z",
          "iopub.status.idle": "2025-04-06T15:30:09.04644Z",
          "shell.execute_reply.started": "2025-04-06T15:30:09.024918Z",
          "shell.execute_reply": "2025-04-06T15:30:09.045585Z"
        },
        "id": "0JRN45ZctgVj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Text Preprocessing"
      ],
      "metadata": {
        "id": "9mE552HHtgVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:14.510283Z",
          "iopub.execute_input": "2025-04-06T15:30:14.510587Z",
          "iopub.status.idle": "2025-04-06T15:30:18.680695Z",
          "shell.execute_reply.started": "2025-04-06T15:30:14.510562Z",
          "shell.execute_reply": "2025-04-06T15:30:18.679681Z"
        },
        "id": "CGHq06tZtgVk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\W', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    words = text.split()\n",
        "    filtered_words = [w for w in words if w not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['clean_text'] = df['text'].apply(clean_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:18.681881Z",
          "iopub.execute_input": "2025-04-06T15:30:18.682183Z",
          "iopub.status.idle": "2025-04-06T15:30:34.241665Z",
          "shell.execute_reply.started": "2025-04-06T15:30:18.682134Z",
          "shell.execute_reply": "2025-04-06T15:30:34.241011Z"
        },
        "id": "5XdbJ-BbtgVl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:34.24307Z",
          "iopub.execute_input": "2025-04-06T15:30:34.24354Z",
          "iopub.status.idle": "2025-04-06T15:30:34.256083Z",
          "shell.execute_reply.started": "2025-04-06T15:30:34.243518Z",
          "shell.execute_reply": "2025-04-06T15:30:34.255185Z"
        },
        "id": "1xJ81SL2tgVl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Tokenization & Padding"
      ],
      "metadata": {
        "id": "m9VCtkKvtgVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['clean_text'])\n",
        "\n",
        "X = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "maxlen = 300\n",
        "X = pad_sequences(X, maxlen=maxlen)\n",
        "\n",
        "y = df['label'].values\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:34.256976Z",
          "iopub.execute_input": "2025-04-06T15:30:34.257196Z",
          "iopub.status.idle": "2025-04-06T15:30:58.041653Z",
          "shell.execute_reply.started": "2025-04-06T15:30:34.257177Z",
          "shell.execute_reply": "2025-04-06T15:30:58.040728Z"
        },
        "id": "pcktzVRYtgVm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab_size\n",
        "# tokenizer\n",
        "# X\n",
        "y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:30:58.042639Z",
          "iopub.execute_input": "2025-04-06T15:30:58.043298Z",
          "iopub.status.idle": "2025-04-06T15:30:58.048595Z",
          "shell.execute_reply.started": "2025-04-06T15:30:58.043265Z",
          "shell.execute_reply": "2025-04-06T15:30:58.047713Z"
        },
        "id": "AwqDIgKQtgVm"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Train-Test-Vald Split"
      ],
      "metadata": {
        "id": "aGWLFig-tgVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: First split off the test set (10%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Step 2: Now split remaining data into train and validation (10% of total = 1/9 of remaining)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1111, random_state=42)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:31:30.950473Z",
          "iopub.execute_input": "2025-04-06T15:31:30.950818Z",
          "iopub.status.idle": "2025-04-06T15:31:31.001788Z",
          "shell.execute_reply.started": "2025-04-06T15:31:30.950761Z",
          "shell.execute_reply": "2025-04-06T15:31:31.000738Z"
        },
        "id": "Xo6nFCnqtgVn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 LSTM Model"
      ],
      "metadata": {
        "id": "iDj7aPmftgVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade --force-reinstall tensorflow"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T10:38:41.991556Z",
          "iopub.execute_input": "2025-04-06T10:38:41.991847Z",
          "iopub.status.idle": "2025-04-06T10:38:41.995306Z",
          "shell.execute_reply.started": "2025-04-06T10:38:41.991825Z",
          "shell.execute_reply": "2025-04-06T10:38:41.994281Z"
        },
        "id": "_00x4JWttgVn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:31:39.104102Z",
          "iopub.execute_input": "2025-04-06T15:31:39.104425Z",
          "iopub.status.idle": "2025-04-06T15:31:39.108Z",
          "shell.execute_reply.started": "2025-04-06T15:31:39.104391Z",
          "shell.execute_reply": "2025-04-06T15:31:39.107082Z"
        },
        "id": "kgRRM_KItgVn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "import os\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, LSTM, Bidirectional, Dense, Dropout\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# Ensure the save directory exists\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n",
        "\n",
        "# Define the models you want to train\n",
        "model_types = ['rnn', 'birnn', 'lstm', 'bilstm']\n",
        "histories = {}\n",
        "results = {}\n",
        "\n",
        "# Build function\n",
        "def build_model(model_type='rnn'):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen))\n",
        "\n",
        "    if model_type == 'rnn':\n",
        "        model.add(SimpleRNN(64, dropout=0.3, recurrent_dropout=0.3))\n",
        "    elif model_type == 'birnn':\n",
        "        model.add(Bidirectional(SimpleRNN(64, dropout=0.3, recurrent_dropout=0.3)))\n",
        "    elif model_type == 'lstm':\n",
        "        model.add(LSTM(64, dropout=0.3, recurrent_dropout=0.3))\n",
        "    elif model_type == 'bilstm':\n",
        "        model.add(Bidirectional(LSTM(64, dropout=0.3, recurrent_dropout=0.3)))\n",
        "\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T10:38:44.101888Z",
          "iopub.execute_input": "2025-04-06T10:38:44.102162Z",
          "iopub.status.idle": "2025-04-06T10:38:44.111058Z",
          "shell.execute_reply.started": "2025-04-06T10:38:44.102141Z",
          "shell.execute_reply": "2025-04-06T10:38:44.110368Z"
        },
        "id": "r3DADKnCtgVn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger\n",
        "\n",
        "# Create directory for saving models\n",
        "os.makedirs(\"saved_models\", exist_ok=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T10:38:47.68076Z",
          "iopub.execute_input": "2025-04-06T10:38:47.681069Z",
          "iopub.status.idle": "2025-04-06T10:38:47.685106Z",
          "shell.execute_reply.started": "2025-04-06T10:38:47.681042Z",
          "shell.execute_reply": "2025-04-06T10:38:47.684252Z"
        },
        "id": "5yiC-iHytgVo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "for mtype in model_types:\n",
        "    print(f\"\\n🔁 Training model: {mtype.upper()}\")\n",
        "\n",
        "    model = build_model(mtype)\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath=f'saved_models/{mtype.upper()}_epoch_{{epoch:02d}}_valacc_{{val_accuracy:.4f}}.keras',\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=False,\n",
        "        verbose=1\n",
        "    )\n",
        "    csv_logger = CSVLogger(f'{mtype.upper()}_training_log.csv', append=True)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=5,\n",
        "        batch_size=64,\n",
        "        callbacks=[checkpoint, csv_logger],\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    scores = model.evaluate(X_val, y_val, verbose=0)\n",
        "    results[mtype] = scores\n",
        "    histories[mtype] = history\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T10:38:50.326999Z",
          "iopub.execute_input": "2025-04-06T10:38:50.327279Z",
          "iopub.status.idle": "2025-04-06T11:34:35.815808Z",
          "shell.execute_reply.started": "2025-04-06T10:38:50.327257Z",
          "shell.execute_reply": "2025-04-06T11:34:35.814895Z"
        },
        "id": "vKzt-MFVtgVo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, average_precision_score, jaccard_score\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "print(confusion_matrix(y_test, y_pred_binary))\n",
        "print(classification_report(y_test, y_pred_binary))\n",
        "\n",
        "mAP = average_precision_score(y_test, y_pred)\n",
        "iou = jaccard_score(y_test, y_pred_binary)\n",
        "\n",
        "print(f\"mAP: {mAP:.4f}\")\n",
        "print(f\"IoU: {iou:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T11:50:02.658436Z",
          "iopub.execute_input": "2025-04-06T11:50:02.65875Z",
          "iopub.status.idle": "2025-04-06T11:50:20.622501Z",
          "shell.execute_reply.started": "2025-04-06T11:50:02.658729Z",
          "shell.execute_reply": "2025-04-06T11:50:20.621605Z"
        },
        "id": "V8bGrh0RtgVo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# List of model types to evaluate\n",
        "model_types = ['RNN_epoch_05_valacc_0.8693', 'BIRNN_epoch_05_valacc_0.6452', 'LSTM_epoch_05_valacc_0.9857', 'BILSTM_epoch_05_valacc_0.9869']\n",
        "histories = {}\n",
        "\n",
        "# Load and evaluate each model\n",
        "for model_type in model_types:\n",
        "    print(f\"\\nEvaluating {model_type} model...\\n\")\n",
        "    # /kaggle/input/all_model/keras/default/1/BILSTM_epoch_05_valacc_0.9869.keras\n",
        "    # Load the best model for the current type\n",
        "    model_path = f'/kaggle/input/all_model/keras/default/1/{model_type}.keras'  # Make sure to adjust the model paths correctly\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Predict using the model\n",
        "    y_pred = model.predict(X_test)  # Assuming X_test is the test data\n",
        "    y_pred_class = (y_pred > 0.5).astype(int)  # Convert probabilities to class labels (0 or 1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred_class)\n",
        "    precision = precision_score(y_test, y_pred_class)\n",
        "    recall = recall_score(y_test, y_pred_class)\n",
        "    f1 = f1_score(y_test, y_pred_class)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_class)\n",
        "\n",
        "    # Print out the results\n",
        "    print(f'Accuracy for {model_type}: {accuracy}')\n",
        "    print(f'Precision for {model_type}: {precision}')\n",
        "    print(f'Recall for {model_type}: {recall}')\n",
        "    print(f'F1-score for {model_type}: {f1}')\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Negative\", \"Positive\"], yticklabels=[\"Negative\", \"Positive\"])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix for {model_type}')\n",
        "    plt.show()\n",
        "\n",
        "    # Store the model's history (if needed for further analysis)\n",
        "    histories[model_type] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1,\n",
        "        'conf_matrix': conf_matrix\n",
        "    }\n",
        "\n",
        "# At this point, you have evaluated all models and plotted their confusion matrices.\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:37:34.132455Z",
          "iopub.execute_input": "2025-04-06T15:37:34.132825Z",
          "iopub.status.idle": "2025-04-06T15:38:30.759318Z",
          "shell.execute_reply.started": "2025-04-06T15:37:34.132765Z",
          "shell.execute_reply": "2025-04-06T15:38:30.75845Z"
        },
        "id": "JG3SkmgotgVo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Text Style Analysis"
      ],
      "metadata": {
        "id": "KLfz_fRutgVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✍️ Feature Engineering\n",
        "### Word Frequency: CountVectorizer or TfidfVectorizer\n",
        "\n",
        "### Sentence Structure: Avg sentence length, punctuation frequency\n",
        "\n",
        "### Sentiment: TextBlob or VADER\n",
        "\n",
        "### Readability: Flesch-Kincaid from textstat"
      ],
      "metadata": {
        "id": "TcMGFCg7tgVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:42:02.338276Z",
          "iopub.execute_input": "2025-04-06T15:42:02.338589Z",
          "iopub.status.idle": "2025-04-06T15:42:06.317763Z",
          "shell.execute_reply.started": "2025-04-06T15:42:02.338569Z",
          "shell.execute_reply": "2025-04-06T15:42:06.316859Z"
        },
        "id": "SaTlXmdXtgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:42:06.319261Z",
          "iopub.execute_input": "2025-04-06T15:42:06.319596Z",
          "iopub.status.idle": "2025-04-06T15:42:09.669597Z",
          "shell.execute_reply.started": "2025-04-06T15:42:06.31957Z",
          "shell.execute_reply": "2025-04-06T15:42:09.668447Z"
        },
        "id": "6gd-jvcytgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:42:09.671344Z",
          "iopub.execute_input": "2025-04-06T15:42:09.67163Z",
          "iopub.status.idle": "2025-04-06T15:42:09.751526Z",
          "shell.execute_reply.started": "2025-04-06T15:42:09.671603Z",
          "shell.execute_reply": "2025-04-06T15:42:09.75064Z"
        },
        "id": "2Tu5wI_JtgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# from textblob import TextBlob\n",
        "# import textstat\n",
        "\n",
        "# def extract_features(text):\n",
        "#     blob = TextBlob(text)\n",
        "#     sentiment = blob.sentiment.polarity\n",
        "#     subjectivity = blob.sentiment.subjectivity\n",
        "#     readability = textstat.flesch_reading_ease(text)\n",
        "#     return pd.Series([sentiment, subjectivity, readability])\n",
        "\n",
        "import textstat\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_text_features(text):\n",
        "    blob = TextBlob(text)\n",
        "    word_count = len(blob.words)\n",
        "    sentence_count = len(blob.sentences)\n",
        "    avg_word_length = sum(len(word) for word in blob.words) / word_count if word_count else 0\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    readability = textstat.flesch_reading_ease(text)\n",
        "\n",
        "    return pd.Series([\n",
        "        word_count,\n",
        "        avg_word_length,\n",
        "        avg_sentence_length,\n",
        "        polarity,\n",
        "        subjectivity,\n",
        "        readability\n",
        "    ])\n",
        "\n",
        "df_features = df['clean_text'].apply(extract_text_features)\n",
        "df_features.columns = [\n",
        "    'word_count', 'avg_word_len', 'avg_sent_len',\n",
        "    'polarity', 'subjectivity', 'readability'\n",
        "]\n",
        "\n",
        "# Add label\n",
        "df_features['label'] = df['label']\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:42:13.779633Z",
          "iopub.execute_input": "2025-04-06T15:42:13.779976Z",
          "iopub.status.idle": "2025-04-06T15:45:00.109228Z",
          "shell.execute_reply.started": "2025-04-06T15:42:13.77995Z",
          "shell.execute_reply": "2025-04-06T15:45:00.108483Z"
        },
        "id": "IAJj6K2ctgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_features"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:45:00.110361Z",
          "iopub.execute_input": "2025-04-06T15:45:00.11067Z",
          "iopub.status.idle": "2025-04-06T15:45:00.127661Z",
          "shell.execute_reply.started": "2025-04-06T15:45:00.11064Z",
          "shell.execute_reply": "2025-04-06T15:45:00.12686Z"
        },
        "id": "67JEAK64tgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Prepare data\n",
        "X_feat = df_features.drop('label', axis=1)\n",
        "y_feat = df_features['label']\n",
        "\n",
        "X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(X_feat, y_feat, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "X_train_feat = scaler.fit_transform(X_train_feat)\n",
        "X_test_feat = scaler.transform(X_test_feat)\n",
        "\n",
        "# Models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier()\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n🧠 Training {name}...\")\n",
        "    model.fit(X_train_feat, y_train_feat)\n",
        "    preds = model.predict(X_test_feat)\n",
        "    print(confusion_matrix(y_test_feat, preds))\n",
        "    print(classification_report(y_test_feat, preds))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:47:46.167851Z",
          "iopub.execute_input": "2025-04-06T15:47:46.168212Z",
          "iopub.status.idle": "2025-04-06T15:47:54.631305Z",
          "shell.execute_reply.started": "2025-04-06T15:47:46.168184Z",
          "shell.execute_reply": "2025-04-06T15:47:54.630383Z"
        },
        "id": "dDYsBcJ9tgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Reshape for LSTM\n",
        "X_train_feat_lstm = X_train_feat.reshape((X_train_feat.shape[0], X_train_feat.shape[1], 1))\n",
        "X_test_feat_lstm = X_test_feat.reshape((X_test_feat.shape[0], X_test_feat.shape[1], 1))\n",
        "\n",
        "# Build LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, input_shape=(X_train_feat_lstm.shape[1], 1), dropout=0.3, recurrent_dropout=0.3),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train\n",
        "lstm_model.fit(X_train_feat_lstm, y_train_feat, epochs=50, batch_size=32, validation_data=(X_test_feat_lstm, y_test_feat))\n",
        "\n",
        "# Evaluate\n",
        "dl_preds = lstm_model.predict(X_test_feat_lstm)\n",
        "dl_preds_binary = (dl_preds > 0.5).astype(int)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(confusion_matrix(y_test_feat, dl_preds_binary))\n",
        "print(classification_report(y_test_feat, dl_preds_binary))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T15:51:38.798659Z",
          "iopub.execute_input": "2025-04-06T15:51:38.799038Z"
        },
        "id": "_uPyH63AtgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Input shape\n",
        "input_shape = X_train_feat.shape[1]\n",
        "\n",
        "# Simple feed-forward NN (we can later wrap it into LSTM if needed)\n",
        "dl_model = Sequential([\n",
        "    Dense(64, input_dim=input_shape, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "dl_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "dl_model.fit(X_train_feat, y_train_feat, epochs=50, batch_size=32, validation_data=(X_test_feat, y_test_feat))\n",
        "\n",
        "# Evaluate\n",
        "dl_preds = dl_model.predict(X_test_feat)\n",
        "dl_preds_binary = (dl_preds > 0.5).astype(int)\n",
        "print(confusion_matrix(y_test_feat, dl_preds_binary))\n",
        "print(classification_report(y_test_feat, dl_preds_binary))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T12:33:57.935012Z",
          "iopub.execute_input": "2025-04-06T12:33:57.935332Z",
          "iopub.status.idle": "2025-04-06T12:35:33.424961Z",
          "shell.execute_reply.started": "2025-04-06T12:33:57.935278Z",
          "shell.execute_reply": "2025-04-06T12:35:33.424181Z"
        },
        "id": "2LX4xUPPtgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dl_model.fit(X_train_feat, y_train_feat, epochs=50, batch_size=32, validation_data=(X_test_feat, y_test_feat))\n",
        "\n",
        "# Evaluate\n",
        "dl_preds = dl_model.predict(X_test_feat)\n",
        "dl_preds_binary = (dl_preds > 0.5).astype(int)\n",
        "print(confusion_matrix(y_test_feat, dl_preds_binary))\n",
        "print(classification_report(y_test_feat, dl_preds_binary))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T12:35:49.089416Z",
          "iopub.execute_input": "2025-04-06T12:35:49.089729Z",
          "iopub.status.idle": "2025-04-06T12:37:20.469503Z",
          "shell.execute_reply.started": "2025-04-06T12:35:49.089706Z",
          "shell.execute_reply": "2025-04-06T12:37:20.46875Z"
        },
        "id": "h-sIRpfStgVp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "dl_model.save(\"style_feature_dl_model.keras\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T12:38:13.516965Z",
          "iopub.execute_input": "2025-04-06T12:38:13.517347Z",
          "iopub.status.idle": "2025-04-06T12:38:13.547255Z",
          "shell.execute_reply.started": "2025-04-06T12:38:13.517286Z",
          "shell.execute_reply": "2025-04-06T12:38:13.546646Z"
        },
        "id": "6I_mViMOtgVq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = load_model(\"style_feature_dl_model.keras\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T12:38:27.18506Z",
          "iopub.execute_input": "2025-04-06T12:38:27.18536Z",
          "iopub.status.idle": "2025-04-06T12:38:27.267993Z",
          "shell.execute_reply.started": "2025-04-06T12:38:27.185336Z",
          "shell.execute_reply": "2025-04-06T12:38:27.267371Z"
        },
        "id": "ZEofM9hVtgVq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import textstat\n",
        "from textblob import TextBlob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model(\"/kaggle/input/style_feature_dl_model/keras/default/1/style_feature_dl_model.keras\")\n",
        "\n",
        "# -------------------------------\n",
        "# Text Feature Extraction Function\n",
        "# -------------------------------\n",
        "def extract_text_features(text):\n",
        "    blob = TextBlob(text)\n",
        "    word_count = len(blob.words)\n",
        "    sentence_count = len(blob.sentences)\n",
        "    avg_word_length = sum(len(word) for word in blob.words) / word_count if word_count else 0\n",
        "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "    readability = textstat.flesch_reading_ease(text)\n",
        "\n",
        "    return np.array([[\n",
        "        word_count,\n",
        "        avg_word_length,\n",
        "        avg_sentence_length,\n",
        "        polarity,\n",
        "        subjectivity,\n",
        "        readability\n",
        "    ]])\n",
        "\n",
        "# -------------------------------\n",
        "# Prediction Pipeline\n",
        "# -------------------------------\n",
        "def predict_fake_real(text):\n",
        "    features = extract_text_features(text)\n",
        "    prediction = model.predict(features)\n",
        "    label = \"FAKE\" if prediction[0][0] > 0.5 else \"REAL\"\n",
        "    confidence = float(prediction[0][0]) if label == \"FAKE\" else 1 - float(prediction[0][0])\n",
        "    return label, confidence\n",
        "\n",
        "# -------------------------------\n",
        "# Example Usage\n",
        "# -------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    user_input = input(\"Enter news text to classify as FAKE or REAL:\\n\")\n",
        "    label, confidence = predict_fake_real(user_input)\n",
        "    print(f\"\\nPredicted Label: {label}\")\n",
        "    print(f\"Confidence Score: {confidence:.2f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-06T16:27:35.160956Z",
          "iopub.execute_input": "2025-04-06T16:27:35.16126Z",
          "iopub.status.idle": "2025-04-06T16:27:42.844753Z",
          "shell.execute_reply.started": "2025-04-06T16:27:35.161232Z",
          "shell.execute_reply": "2025-04-06T16:27:42.843888Z"
        },
        "id": "HTqH546-tgVq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kxx5hgWGtgVq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "oH9iovOdtgVq"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}